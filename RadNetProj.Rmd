---
title: "RadNet Exploratory Data Analysis"
output: html_notebook
---
RadNet Data by Wendy Bisset
========================================================


> **Tip**: You will see quoted sections like this throughout the template to
help you construct your report. Make sure that you remove these notes before
you finish and submit your project!

> **Tip**: One of the requirements of this project is that your code follows
good formatting techniques, including limiting your lines to 80 characters or
less. If you're using RStudio, go into Preferences \> Code \> Display to set up
a margin line to help you keep track of this guideline!

```{r echo=FALSE, message=FALSE, warning=FALSE, packages}
# Load all of the packages that you end up using in your analysis in this code
# chunk.
# Notice that the parameter "echo" was set to FALSE for this code chunk. This
# prevents the code from displaying in the knitted HTML output. You should set
# echo=FALSE for all code chunks in your file, unless it makes sense for your
# report to show the code that generated a particular plot.

# The other parameters for "message" and "warning" should also be set to FALSE
# for other code chunks once you have verified that each plot comes out as you
# want it to. This will clean up the flow of your report.

library(ggplot2)
library(dplyr)
library(ggmap)

citation('ggmap')
```

#Data Source and Info

>RadNet is a national network of more than 200 monitoring stations distributed across all 50 states and the American Territories. These stations regularly sample the nation's air, precipitation, or drinking water for a variety of radionuclides (e.g., iodine-131) and radiation types (e.g., gross beta (Î²)). During its operation beginning in1973, RadNet's predecessor, the Environmental Radiation Ambient Monitoring System (ERAMS), collected over a half million high quality environmental samples. The current database primarily provides data that was collected between 1978 and present.. Some older "pre-ERAMS" data is included, and additional data will be added soon

Data downloaded directly from website [RadNet](https://www.epa.gov/enviro/radnet-customized-search) using 3 tables:
- Media
- Sampling Location
- Types of Radionuclides and Radiation
Data was downloaded in decade chunks for data pre-1989, 1990-1999, 2000-2009, 2010-2017 for 5 media types:
+ drinking water (DW)
+ surface water (SW) program was terminated March 1999
+ precipitation
+ milk program was terminated 2014
+ air data combined
  air-charcoal: data only found in 2010-2017 query
  air-filter: data all years
No filters or limitations were placed on location or types of radionuclides analysed. Some columns (variables) were filtered out and will not be examined in this project. ()

```{r}
#list files in Data Folder 
list.files('radnet_data/', recursive = TRUE)
```
#Loading DATA and Merging Data Files

As the data was downloaded in chunks (the website mentioned a 10,000 record limit to download)*ref needed* the resulting .csv files must be consolidated. 
REFERENCE:  [Merge Files into single DataFrame](https://psychwire.wordpress.com/2011/06/03/merge-all-files-in-a-directory-using-r-into-a-single-dataframe/)

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_the_Data}
# Load/Merge the Data
# delete existing dataframes if rerunning this chunk otherwise duplicates will result
dir_list <-  list.files('radnet_data/')

for (directory in dir_list){
  subdir <- (paste('radnet_data/', directory, sep=""))
  files_in_sub <- list.files(subdir)
  dfname <- paste(directory, '_data', sep="")  
  
  for (file in files_in_sub){
    file_loc <- paste(subdir, '/', file, sep="")
    print(file_loc)
    # if the merged dataset doesn't exist, create it
    if (!exists("db")){
        db <- read.csv(file_loc, header=TRUE)
    }
    # if the merged dataset does exist, append to it
    else if (exists("db")){
        temp_dataset <-read.csv(file_loc, header=TRUE)
        db <-rbind(db, temp_dataset)
       rm(temp_dataset)
    }
  }
  assign(dfname, db)
  remove(db)
}

```

[Getting List of data.frame names](http://r.789695.n4.nabble.com/getting-list-of-data-frame-names-td3864338.html)
While one can create character list of dataframes (listofDF), using this list in bind_rows does not work.  passing listofDF passes "char" not object - error generated
listofDF <- as.list(names(which(sapply(.GlobalEnv, is.data.frame))))

```{r echo=FALSE, message=FALSE, warning=FALSE, tidy_data}
#SAMP_ID iS INT IN surface_water_data and CHR in all other dataframes convert before 
#df consolidation into one dataframe
surface_water_data$V_ERAMS_MATRIX_SAMPLE_ANALYSIS.SAMP_ID <-
  as.character(
    surface_water_data$V_ERAMS_MATRIX_SAMPLE_ANALYSIS.SAMP_ID)

rad_data <- bind_rows(list(air_data, drinking_water_data, Milk_data,
                           precipitation_data, surface_water_data),
                      .id ="id")
```
# Data Overview 
## Cleanup/Shorten Variable names

```{r tidy_data_colnames}
colnames(surface_water_data) <-gsub('.+\\.','',colnames(surface_water_data))
colnames(Milk_data) <-gsub('.+\\.','',colnames(Milk_data))
colnames(air_data) <-gsub('.+\\.','',colnames(air_data))
colnames(precipitation_data) <-gsub('.+\\.','',colnames(precipitation_data))
colnames(drinking_water_data)<-
  gsub('.+\\.','',colnames(drinking_water_data))
colnames(rad_data) <-gsub('.+\\.','',colnames(rad_data))
```
# Recast Some Character Variables as Factors or Ordered Factors
some variables have limited categories, such as State abbreviations, Analytes, or the measurement units these can be addressed as factors and in the case of units it may be possible to impose an order, e.g., mL<L; recast Date field from chr to date
[as.Date](https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/as.Date)
```{r dataframe_info}

#test factorization on small dataframe
#surface_water_data$MATERIAL <- factor(surface_water_data$MAT_ID)
#surface_water_data$STATE <- factor(surface_water_data$STATE_ABBR)
#str(surface_water_data$STATE)
#str(surface_water_data$MATERIAL)

## recast variables in rad_data (consolidated) dataframe

rad_data$ID <- factor(as.integer(rad_data$id))
rad_data$MAT_ID <- factor(rad_data$MAT_ID)
rad_data$ANA_UNIT <- factor(rad_data$ANA_UNIT)
rad_data$ANA_PROC_NUM <- factor(rad_data$ANA_PROC_NUM)
rad_data$LOC_NUM <- factor(rad_data$LOC_NUM)
rad_data$CITY_NAME <- factor(rad_data$CITY_NAME)
rad_data$STATE_ABBR <- factor(rad_data$STATE_ABBR)
rad_data$ANALYTE_ID <- factor(rad_data$ANALYTE_ID)
rad_data$RESULT_UNIT <- factor(rad_data$RESULT_UNIT)
rad_data$ANA_TYPE <- factor(rad_data$ANA_TYPE)
rad_data$HALF_LIFE_TIME_UNIT <- factor(rad_data$HALF_LIFE_TIME_UNIT)
rad_data$ANALYTE_NAME <- factor(rad_data$ANALYTE_NAME)
rad_data[1] <- NULL 
# where %F == "%Y-%m-%d" ISO 8601 format
rad_data$RESULT_DATE <- as.Date(rad_data$RESULT_DATE, "%F")
summary(rad_data$RESULT_DATE)

```
```{r}
summary(rad_data$ANA_UNIT)
summary(rad_data$RESULT_UNIT)
summary(rad_data$HALF_LIFE_TIME_UNIT)
##Create ordered factor for Time units sec<min<hour<day<year
rad_data$HALF_LIFE_TIME_UNIT <- factor(rad_data$HALF_LIFE_TIME_UNIT,
                                       levels  = c("S", "M", 
                                                   "H", "D", "Y"))
levels(rad_data$HALF_LIFE_TIME_UNIT)
```


```{r}
##info for rad_data
head(rad_data)
dim(rad_data)
names(rad_data)
summary(rad_data)
```

##attempt to convert SAMP_ID to numeric, 
some SAMP_ID values are prefaced with alpha character
```{r}
rad_data$sample <- as.numeric(rad_data$SAMP_ID)
summary(rad_data$sample)

Milk_data %>% 
            select(SAMP_ID, RESULT_AMOUNT, LOC_NUM) %>%     
            filter(is.na(as.numeric(SAMP_ID))) %>% 
            count(SAMP_ID)
str(rad_data$SAMP_ID)
```

# Geocoding locations
Use library ggmap loaded in library chunk, line 32 to obtain LAT/LON of unique locations

[How to geocode a csv of addresses in R](http://www.storybench.org/geocode-csv-addresses-r/)
[geocode](https://www.rdocumentation.org/packages/ggmap/versions/2.6.1/topics/geocode)
[Package 'ggmap'](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)
```{r}
#cities <- unique(rad_data$CITY_NAME)
##cities
#states <-unique(rad_data$STATE_ABBR)
#statelist <- c('NM', 'VA', 'CA', 'NE')
#listofDF[1]
```
#Test geocode
Test returns lat/lon of first row in dataframe
```{r}
citystate <- paste(rad_data[1,which( colnames(rad_data)=="STATE_ABBR")],
                   rad_data[1,which( colnames(rad_data)=="CITY_NAME")])
geocode(citystate, output = "latlona", source="dsk")
geocode("Honolulu HI", output = "latlona", source="dsk")
```
#Make Unique list of CityState for geocoding
as.character returns number instead of string
to convert "list" to character 
https://stackoverflow.com/questions/20336557/as-character-returning-numbers-instead-of-strings-r
```{r}
locations <- data.frame("LOCNUM" = rad_data$LOC_NUM, 
                        "CITY" = rad_data$CITY_NAME,   
                        "STATE" =rad_data$STATE_ABBR)
#length(unique(locations$LOCNUM))
locations <- unique(locations)
locations <- arrange(locations, LOCNUM)

##Fix locaation errors:
##Abbrieviation 'PC'is not found in geocode(), change STATE to 'Panama' 
##One entry miscoded as Doswell SC should be Doswell VA 
##confirmed by comparison of LOC_NUM, previously these were fixed after geocode()
locations$STATE <- as.character(locations$STATE)
locations$STATE[(locations$STATE == "PC")] <- "Panama"
locations$STATE[(locations$STATE == "SC") & (locations$CITY == "DOSWELL")] <- "VA"


locations$citystate <- paste(locations$CITY,
                             locations$STATE)

locations$lat <- NA
locations$lon <- NA
locations$address <- NA

#Use tryCatch with geocode()
#to bypass error from EPA Regions which have no specific LAT/LON
for (i in 1:nrow(locations)) {
  result <- tryCatch(geocode(locations[i,4], output = c("latlona"), source="dsk"),
                      warning = function(w) {
                        print("warning");
                        locations[i,5] <- NA
                        locations[i,6] <- NA  
                        locations[i,7] <- NA
                      },
                      error = function(e) {
                          print("error");
                          next
                      })
  locations[i,5] <- as.numeric(result[2])
  locations[i,6] <- as.numeric(result[1])
  locations[i,7] <- as.character(unlist(result[3]))
}
locations$lat <- as.numeric(locations$lat)
##Revised latlong from latlong.net - geocoding put Honolulu in ocean on Kauia on the ##wrong islan
locations$lat[locations$CITY == "HONOLULU"] = 21.306944
locations$lon[locations$CITY == "HONOLULU"] = -157.858333
locations$lat[locations$CITY == "KAUAI"] = 22.096440
locations$lon[locations$CITY == "KAUAI"] = -159.526124
```
##How to handle [Errors from Geocode](https://stackoverflow.com/questions/30770328/how-to-handle-error-from-geocode-ggmap-r)

>loc = 'Blue Grass Airport'
>x <- tryCatch(geocode(loc, output = c("more")),
              warning = function(w) {
                            print("warning"); 
                            # handle warning here
                        },
              error = function(e) {
                          print("error");
                          # handle error here
                      })
      
      
                      
##Graphic Visualization with R's ggmap
https://blog.dominodatalab.com/geographic-visualization-with-rs-ggmaps/
--------
```{r}
#qmap(location = "New Mexico State University")
usa_center = as.numeric(geocode("United States"))
USAMap = ggmap(get_googlemap(center=usa_center, scale=2, zoom=4), extent="normal")
USAMap
continental <- locations %>% filter(lon < -75 & lon > -120 & 
                                      lat < 50 & lat > 20)
USAMap + geom_point(data = continental, aes(x= lon, y= lat),
                    col="blue", alpha=0.4) +
  scale_y_continuous(limits = c(10,55))
  
```


```{r}
#qmplot(lon, lat, data = locations)

us <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(us, zoom = 5, maptype = "toner-lite")
ggmap(map)
```


```{r}

ggmap(map) + geom_point(data = continental, aes(x= lon, y= lat),
                    col="blue", alpha=0.4)
```

```{r}
HI_center = as.numeric(geocode("Hawaii"))
HI_center = c(-157.9, 21.2)
HIMap = ggmap(get_googlemap(center=HI_center, zoom=7, extent="normal"))
HIMap

hawaii <- locations %>% filter(lon < -154 & lon > -161 & 
                                      lat < 23 & lat > 17)
HIMap  + geom_point(data = hawaii, aes(x= lon, y= lat),
                    col="red", alpha=0.9)
HI_data <- rad_data %>% filter(STATE_ABBR == "HI")
#bounding box HI 
HI <- c(left = -161, bottom = 17, right = -154, top = 23)
HImap <- get_stamenmap(HI, zoom = 6, maptype = "toner-lite")
ggmap(HImap)
```




> **Tip**: Before you create any plots, it is a good idea to provide a short
introduction into the dataset that you are planning to explore. Replace this
quoted text with that general information!

# Univariate Plots Section

> **Tip**: In this section, you should perform some preliminary exploration of
your dataset. Run some summaries of the data and create univariate plots to
understand the structure of the individual variables in your dataset. Don't
forget to add a comment after each plot or closely-related group of plots!
There should be multiple code chunks and text sections; the first one below is
just to help you get started.

```{r echo=FALSE, Univariate_Plots}
ggplot(HI_data, aes(table, price, color = cut)) +
    geom_point()+
    scale_color_brewer(type='qual')+
  scale_x_continuous(name ="Table", breaks=seq(50,80,2), limits = c(50,80))
```

> **Tip**: Make sure that you leave a blank line between the start / end of
each code block and the end / start of your Markdown text so that it is
formatted nicely in the knitted text. Note as well that text on consecutive
lines is treated as a single space. Make sure you have a blank line between
your paragraphs so that they too are formatted for easy readability.

# Univariate Analysis

> **Tip**: Now that you've completed your univariate explorations, it's time to
reflect on and summarize what you've found. Use the questions below to help you
gather your observations and add your own if you have other thoughts!

### What is the structure of your dataset?

### What is/are the main feature(s) of interest in your dataset?

### What other features in the dataset do you think will help support your \
investigation into your feature(s) of interest?

### Did you create any new variables from existing variables in the dataset?

### Of the features you investigated, were there any unusual distributions? \
Did you perform any operations on the data to tidy, adjust, or change the form \
of the data? If so, why did you do this?


# Bivariate Plots Section

> **Tip**: Based on what you saw in the univariate plots, what relationships
between variables might be interesting to look at in this section? Don't limit
yourself to relationships between a main output feature and one of the
supporting variables. Try to look at relationships between supporting variables
as well.

```{r echo=FALSE, Bivariate_Plots}

```

# Bivariate Analysis

> **Tip**: As before, summarize what you found in your bivariate explorations
here. Use the questions below to guide your discussion.

### Talk about some of the relationships you observed in this part of the \
investigation. How did the feature(s) of interest vary with other features in \
the dataset?

### Did you observe any interesting relationships between the other features \
(not the main feature(s) of interest)?

### What was the strongest relationship you found?


# Multivariate Plots Section

> **Tip**: Now it's time to put everything together. Based on what you found in
the bivariate plots section, create a few multivariate plots to investigate
more complex interactions between variables. Make sure that the plots that you
create here are justified by the plots you explored in the previous section. If
you plan on creating any mathematical models, this is the section where you
will do that.

```{r echo=FALSE, Multivariate_Plots}

```

# Multivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. Were there features that strengthened each other in terms of \
looking at your feature(s) of interest?

### Were there any interesting or surprising interactions between features?

### OPTIONAL: Did you create any models with your dataset? Discuss the \
strengths and limitations of your model.

------

# Final Plots and Summary

> **Tip**: You've done a lot of exploration and have built up an understanding
of the structure of and relationships between the variables in your dataset.
Here, you will select three plots from all of your previous exploration to
present here as a summary of some of your most interesting findings. Make sure
that you have refined your selected plots for good titling, axis labels (with
units), and good aesthetic choices (e.g. color, transparency). After each plot,
make sure you justify why you chose each plot by describing what it shows.

### Plot One
```{r echo=FALSE, Plot_One}

```

### Description One


### Plot Two
```{r echo=FALSE, Plot_Two}

```

### Description Two


### Plot Three
```{r echo=FALSE, Plot_Three}

```

### Description Three

------

# Reflection

> **Tip**: Here's the final step! Reflect on the exploration you performed and
the insights you found. What were some of the struggles that you went through?
What went well? What was surprising? Make sure you include an insight into
future work that could be done with the dataset.

> **Tip**: Don't forget to remove this, and the other **Tip** sections before
saving your final work and knitting the final report!
