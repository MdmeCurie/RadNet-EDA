---
title: "RadNet Exploratory Data Analysis"
output:
  html_notebook: default
  pdf_document: default
  editor_options: 
    chunk_output_type: inline
    chunk_output_type: console
---
*by Wendy Bisset*
========================================================

```{r echo=FALSE, message=FALSE, warning=FALSE, packages}
# Load all of the packages in this code chunk
# Parameter "echo" = FALSE prevents the code from displaying in the knitted #HTML output. 
# Set echo=FALSE for all code chunks in file, unless it makes sense to show #the code that generated a particular plot.
# Parameters for "message" and "warning" should also be set to FALSE
# for other code chunks once you have verified that each plot comes out as #you want it to. This will clean up the flow of your report.

library(ggplot2)
library(dplyr)
library(ggmap)

## citation('ggmap')
##To cite ggmap in publications, please use:
##[^ggmap]:D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, ##5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf
```

#Data Source and Background Information

The US Environmental Protection Agency maintains several public databases on environmental data.[^EPA]:<https://www.epa.gov/enviro/envirofacts-overview> One such database, ***RadNet***,[^RadNetOverview]:<https://www.epa.gov/enviro/radnet-overview> is a system of geographically distributed stations which monitor a number of radiochemical analytes (e.g., iodine-131, gross beta (Î²) in the The nation's air, precipitation, and drinking water. These stations are located across the  US as well as American Territories. The current RadNet database primarily consists of data collected since 1978 though some older  from RadNet's precursor, ERAMS (Environmental Radiation Ambient Monitoring System), which started operations in 1973 is included.

While the stations of RadNet normally sample on a regular schedule, if there is concern for a significant radiation release the frequency of sampling rate can be increased. For example, there have been five times in which the sampling rates were increased: 1) 1979 Three Mile Island nuclear reactor accident in the U.S., 2) 1986 Chernobyl nuclear reactor accident in the Soviet Union, 3) 1999 Tokaimura nuclear fuel processing facility accident in Japan, 4) 2000 wildfires in Los Alamos and Hanford U.S., and 5) 2001 terrorist attacks in the U.S.

RadNet helps provide historical data needed to estimate long-term trends in environmental radiation levels and also provides a means to estimate levels of radioactivity in the environment. This data helps establish background radiation as well as identify radioactive fallout from atomic weapons testing, nuclear accidents, or other intrusions of radioactive materials. There is a timeline on the RadNet website[^RadNetTimeline]:<https://www.epa.gov/radnet/history-radnet> which lists changes in the monitoring system and events that may have impacted the measurements.  

#Downloading, Importing and Merging Data Files

All data was downloaded directly from the [RadNet](https://www.epa.gov/enviro/radnet-customized-search) website.
According to the EPA's [Envirofacts Data Service API](https://www.epa.gov/enviro/web-services) data output is limited to 10000 rows of data at a time from a maximum of three tables. Consequently, it was decided to download the RadNet data in decade chunks (up-to 1989, 1990-1999, 2000-2009, 2010-2017) and  [merge](https://psychwire.wordpress.com/2011/06/03/merge-all-files-in-a-directory-using-r-into-a-single-dataframe/) resulting .csv files into a single R Dataframe.

The Database Tables Chosen from which to Export RadNet Data were:

1. Media
   + Drinking Water (DW)
   + Surface Water (SW) program was terminated March 1999
   + Precipitation
   + Milk (program was terminated 2014)
   + Air Data combined from: 
     - Air-Charcoal (data only found in 2010-2017)
     - Air-Filter
2. Sampling Location
   + 324 Location IDs across US and Territories
3. Types of Radionuclides and Radiation
   + 61 analytes (e.g., )

In total 19 variables were downloaded which, beyond the media types listed above, include location fields (ID number, city, state), analyte, analyte half-life, analyte procedure number and result amount.  Variables also include units of measure, e.g. size of sample (g, L) or time of decay (sec, min, hour, day, year). No filters or limitations were placed on location or types of radionuclides analysed. Other variables such as project numbers, surface water locations, sample collection start and end etc., exist in the RadNet database, but in effort to to control file size and project scope, these data were not downloaded.

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_Data}
##!! DELETE existing dataframes if rerunning this chunk otherwise data will be re-appended

## List files in Data Folder 
## list.files('radnet_data/', recursive = TRUE)

##Load/Merge the Data into 5 data frames
dir_list <-  list.files('radnet_data/')

dirfor (directory in dir_list){
  subdir <- (paste('radnet_data/', directory, sep=""))
  files_in_sub <- list.files(subdir)
  dfname <- paste(directory, '_data', sep="")  
  
  for (file in files_in_sub){
    file_loc <- paste(subdir, '/', file, sep="")
    # print(file_loc)
    # if the merged dataset doesn't exist, create it
    if (!exists("db")){
        db <- read.csv(file_loc, header=TRUE)
    }
    # if the merged dataset does exist, append to it
    else if (exists("db")){
        temp_dataset <-read.csv(file_loc, header=TRUE)
        db <-rbind(db, temp_dataset)
       rm(temp_dataset)
    }
  }
  assign(dfname, db)
  remove(db)
}
##Cleanup Workspace
rm(dfname, dir_list, directory, file, file_loc, files_in_sub, subdir)

```

It was attempted to bind the media datasets using a loop, but it was discovered that getting a list of dataframe names using `listofDF <- as.list(names(which(sapply(.GlobalEnv, is.data.frame))))`[^df_names]: <http://r.789695.n4.nabble.com/getting-list-of-data-frame-names-td3864338.html>
generates a  character list of dataframes (listofDF). Using this list in bind_rows passes CHAR representation of the dataframe and not the object consequently an error is generated. Thus the dataframe names were hard coded into the argument of `bind_rows()`.

```{r echo=FALSE, message=FALSE, warning=FALSE, tidy_data}
#SAMP_ID iS INT in surface_water_data and CHR in all other dataframes #convert before df consolidation into one dataframe

surface_water_data$V_ERAMS_MATRIX_SAMPLE_ANALYSIS.SAMP_ID <-
  as.character(
    surface_water_data$V_ERAMS_MATRIX_SAMPLE_ANALYSIS.SAMP_ID)
#consolidate media data into one rad_data_raw dataframe
rad_data_raw <- bind_rows(list(air_data, drinking_water_data, Milk_data,
                           precipitation_data, surface_water_data),
                         .id ="id")
```

# Data Cleanup and Wrangling
#### (@)Cleanup/Shorten Variable names
All the variables were prepended with the name of SQL tables frome which they originated, e.g. V_ERAMS_MATRIX_SAMPLE_ANALYSIS.MAT_ID.  All characters up to and including the period are removed. 

```{r echo=FALSE, tidy_data_colnames}
colnames(surface_water_data) <-gsub('.+\\.','',colnames(surface_water_data))
colnames(Milk_data) <-gsub('.+\\.','',colnames(Milk_data))
colnames(air_data) <-gsub('.+\\.','',colnames(air_data))
colnames(precipitation_data) <-gsub('.+\\.','',colnames(precipitation_data))
colnames(drinking_water_data)<-
  gsub('.+\\.','',colnames(drinking_water_data))
colnames(rad_data_raw) <-gsub('.+\\.','',colnames(rad_data_raw))
```

#### (@)Recast Variables Types 
Some variables have limited categories, such as state abbreviations, analytes, or  measurement units that can be addressed as factors. In the case of measurement units it may be possible to also impose an order, e.g., mL < L. The date field is also recast as DATE instead of CHR.
[^as.Date]:<https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/as.Date>

```{r echo=FALSE, variable_info}
#data types and structure 
str(rad_data_raw$ANA_PROC_NUM)
table(rad_data_raw$ANA_UNIT)
table(rad_data_raw$RESULT_UNIT)
table(rad_data_raw$HALF_LIFE_TIME_UNIT)
table(rad_data_raw$ANA_TYPE)
n_distinct(rad_data_raw$LOC_NUM)
n_distinct(rad_data_raw$CITY_NAME)
n_distinct(rad_data_raw$STATE_ABBR)
n_distinct(rad_data_raw$ANALYTE_ID)
n_distinct(rad_data_raw$ANALYTE_NAME)

ggplot(rad_data_raw, aes(LOC_NUM)) + 
  geom_histogram(bins = 50)

str(rad_data_raw$LOC_NUM)
```

```{r echo=FALSE, dataframe_info}
## Recast variables in final "rad_data" (consolidated) dataframe
#ID for original df source (numerical tie to media type)
rad_data <- rad_data_raw
rad_data$ID          <- factor(as.integer(rad_data_raw$id))  

#media type e.g. Air, SW, DW, PPT, MILK
rad_data$MAT_ID      <- factor(rad_data_raw$MAT_ID) 
#size of sample G,L,M3, MG, ML
rad_data$ANA_UNIT    <- factor(rad_data_raw$ANA_UNIT)       
# 61 different Analytes
rad_data$ANALYTE_NAME <- factor(rad_data$ANALYTE_NAME)
# 5 Result measures: ACI/M3, DPM/GC, G/L, PCI/L, PCI/M3
rad_data$RESULT_UNIT <- factor(rad_data$RESULT_UNIT)
# 2 Analyte types E **(?)** and R (radioactive)
rad_data$ANA_TYPE    <- factor(rad_data$ANA_TYPE)
# Five time Measures of Half Life: S, M, H, D, Y
##Create ordered factor for Time units sec<min<hour<day<year
rad_data$HALF_LIFE_TIME_UNIT <- factor(rad_data$HALF_LIFE_TIME_UNIT, 
                                       levels = c("S", "M", "H", "D", "Y"))

##Other potential factor variables                                       
# Location ID (324 unique IDs)
#rad_data$LOC_NUM     <- factor(rad_data$LOC_NUM)
#City Name 289 different cities
#rad_data$CITY_NAME   <- factor(rad_data$CITY_NAME)
#State IDs, 67 different states
#rad_data$STATE_ABBR  <- factor(rad_data$STATE_ABBR)
# 61 different Analytes ANALYTE_ID related to ANALYTE_NAME
#rad_data$ANALYTE_ID  <- factor(rad_data$ANALYTE_ID)

#remove id as col[1], ID is still in col[20]
rad_data[1] <- NULL 
# Set DATE to ISO format, where %F == "%Y-%m-%d" ISO 8601 format
rad_data$RESULT_DATE <- as.Date(rad_data$RESULT_DATE, "%F")
summary(rad_data$RESULT_DATE)

```
```{r echo=FALSE}
summary(rad_data$ANA_UNIT)
summary(rad_data$RESULT_UNIT)
summary(rad_data$HALF_LIFE_TIME_UNIT)
levels(rad_data$HALF_LIFE_TIME_UNIT)
```


```{r echo=FALSE}
##info for rad_data
head(rad_data)
dim(rad_data)
dim(rad_data_raw)
names(rad_data)
summary(rad_data)
```

###Attempt to convert SAMP_ID to numeric, 
In order to easily handle samples and possibly compare multiple results on the same sample it was attempted to convert the SAMP_ID to numeric type, however, some SAMP_ID values are prefaced with alpha character.  

```{r echo=FALSE}
rad_data$sample <- as.numeric(rad_data$SAMP_ID)
summary(rad_data$sample)

Milk_data %>% 
            select(SAMP_ID, RESULT_AMOUNT, LOC_NUM) %>%     
            filter(is.na(as.numeric(SAMP_ID))) %>% 
            count(SAMP_ID)
str(rad_data$SAMP_ID)
```

#Geocoding Locations
Using the package `ggmap` to obtain LAT/LON of locations in the dataframe.  First a 

* [How to geocode a csv of addresses in R](http://www.storybench.org/geocode-csv-addresses-r/)  
* [geocode](https://www.rdocumentation.org/packages/ggmap/versions/2.6.1/topics/geocode)  
* [Package 'ggmap'](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)  

```{r echo=FALSE}
#Test `geocode()`,  returns lat/lon and address of first row in dataframe
citystate <- paste(rad_data[1,which( colnames(rad_data)=="STATE_ABBR")],
                   rad_data[1,which( colnames(rad_data)=="CITY_NAME")])
geocode(citystate, output = "latlona", source="dsk")
geocode("Honolulu HI", output = "latlona", source="dsk")
```



```{r echo=FALSE}
#Unique list of City/State for geocoding
#Use as.character to return number instead of string
#To convert "list" to character 
#<https://stackoverflow.com/questions/20336557/as-character-returning-numbers-instead-of-strings-r>

locations <- data.frame("LOCNUM" = rad_data$LOC_NUM, 
                        "CITY" = rad_data$CITY_NAME,   
                        "STATE" =rad_data$STATE_ABBR)
#length(unique(locations$LOCNUM))
locations <- unique(locations)
locations <- arrange(locations, LOCNUM)

##Fix location errors:
##Abbrieviation 'PC' is not found in geocode(), change STATE to 'Panama' 
##One entry miscoded as Doswell SC should be Doswell VA 
##confirmed by comparison of LOC_NUM, previously these were found/fixed   ##after running geocode()
locations$STATE <- as.character(locations$STATE)
locations$STATE[(locations$STATE == "PC")] <- "Panama"
locations$STATE[(locations$STATE == "SC") & (locations$CITY == "DOSWELL")] <- "VA"


locations$citystate <- paste(locations$CITY,
                             locations$STATE)

locations$lat <- NA
locations$lon <- NA
locations$address <- NA

#Use tryCatch with geocode()
#to bypass error from EPA Regions which have no specific LAT/LON
for (i in 1:nrow(locations)) {
  result <- tryCatch(geocode(locations[i,4], output = c("latlona"), source="dsk"),
                      warning = function(w) {
                        print("warning");
                        locations[i,5] <- NA
                        locations[i,6] <- NA  
                        locations[i,7] <- NA
                      },
                      error = function(e) {
                          print("error");
                          next
                      })
  locations[i,5] <- as.numeric(result[2])
  locations[i,6] <- as.numeric(result[1])
  locations[i,7] <- as.character(unlist(result[3]))
}
locations$lat <- as.numeric(locations$lat)
##Revised latlong from latlong.net - geocoding put Honolulu in ocean and ##Kauia on the wrong island as found during mapping
locations$lat[locations$CITY == "HONOLULU"] = 21.306944
locations$lon[locations$CITY == "HONOLULU"] = -157.858333
locations$lat[locations$CITY == "KAUAI"] = 22.096440
locations$lon[locations$CITY == "KAUAI"] = -159.526124
```
##How to handle [Errors from Geocode](https://stackoverflow.com/questions/30770328/how-to-handle-error-from-geocode-ggmap-r)

`loc = 'Blue Grass Airport'
x <- tryCatch(geocode(loc, output = c("more")),
              warning = function(w) {
                            print("warning"); 
                            # handle warning here
                        },
              error = function(e) {
                          print("error");
                          # handle error here
                      })`
      
      
                      
##Graphic Visualization with R's ggmap

<https://blog.dominodatalab.com/geographic-visualization-with-rs-ggmaps/>
--------

```{r echo=FALSE}
#qmap(location = "New Mexico State University")
usa_center = as.numeric(geocode("United States"))
USAMap = ggmap(get_googlemap(center=usa_center, scale=2, zoom=4), extent="normal")
USAMap
continental <- locations %>% filter(lon < -75 & lon > -120 & 
                                      lat < 50 & lat > 20)
USAMap + geom_point(data = continental, aes(x= lon, y= lat),
                    col="blue", alpha=0.4) +
  scale_y_continuous(limits = c(10,55))
  
```


```{r echo=FALSE}
#qmplot(lon, lat, data = locations)

us <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(us, zoom = 5, maptype = "toner-lite")
ggmap(map) + geom_point(data = continental, aes(x= lon, y= lat),
                    col="blue", alpha=0.4)
```



```{r echo=FALSE, HAWAII}
#HI_center = as.numeric(geocode("Hawaii"))
HI_center = c(-157.9, 21.2)
HIMap = ggmap(get_googlemap(center=HI_center, zoom=7, extent="normal"))
HIMap

hawaii <- locations %>% filter(lon < -154 & lon > -161 & 
                                      lat < 23 & lat > 17)
HIMap  + geom_point(data = hawaii, aes(x= lon, y= lat),
                    col="red", alpha=0.9)
HI_data <- rad_data %>% filter(STATE_ABBR == "HI")
```

```{r}
#bounding box HI 
HI <- c(left = -161, bottom = 17, right = -154, top = 23)
HImap <- get_stamenmap(HI, zoom = 6, maptype = "toner-lite")
ggmap(HImap) + geom_point(data = hawaii, aes(x= lon, y= lat),
                    col="red", alpha=0.9)
```


# Univariate Plots Section

> **Tip**: In this section, you should perform some preliminary exploration of your dataset. Run some summaries of the data and create univariate plots to understand the structure of the individual variables in your dataset. Don't
forget to add a comment after each plot or closely-related group of plots!
There should be multiple code chunks and text sections; the first one below is just to help you get started.

```{r}
#How Many Region Values are there?
regions <- filter(rad_data, grepl('REGION', rad_data$CITY_NAME))
table(regions$MAT_ID)

```

```{r}
#OVERVIEW OF HI DATA
#Remove observations with no RESULT_AMOUNT
count(HI_data, !is.na(HI_data$RESULT_AMOUNT))
HI_data_filt <- HI_data[complete.cases(HI_data[,11]),]
count(HI_data_filt, !is.na(HI_data_filt$RESULT_AMOUNT))

table(droplevels(HI_data_filt$ANALYTE_NAME))
ggplot(HI_data_filt, aes(RESULT_DATE, fill = MAT_ID)) + 
  geom_histogram(bins = 50, alpha = 0.6)

ggplot(HI_data_filt, aes(RESULT_DATE, fill = ANALYTE_NAME)) + 
  geom_histogram(bins = 50, alpha = 0.6)

ggplot(subset(HI_data_filt, (
                              RESULT_UNIT == 'PCI/L' & 
                              MAT_ID == 'PASTEURIZED MILK')),
              aes(RESULT_DATE, RESULT_AMOUNT)) +
        geom_point()

length(unique(HI_data_filt$SAMP_ID))
str(HI_data_filt$SAMP_ID)
```
Of the 504,092 observations in the combined data 81,921 do not actually have a result amount 
```{r echo=FALSE, Univariate_Plots}

# Find observations without Results 81,291 of the 504,092 observations have
# NA for RESULT AMOUNT
count(rad_data, !is.na(rad_data$RESULT_AMOUNT))
head(filter(rad_data, is.na(rad_data$RESULT_AMOUNT)), 3)

# Filter out the NA rows
rad_data_filt <- rad_data[complete.cases(rad_data[,"RESULT_AMOUNT"]),]
# count(rad_data_filt, !is.na(rad_data_filt$RESULT_AMOUNT))

# Count the responses based on the categories within the 
# different variables, top counts listed in comments
count(rad_data_filt, MAT_ID)       # Predominantly AIR-Filter 244,590      
count(rad_data_filt, ANALYTE_NAME) # Mostly Gross Beta 232,387
count(rad_data_filt, ANA_PROC_NUM) # Procedure #1 217744
count(rad_data_filt, CITY_NAME)    # varied top 5000- 8000
count(rad_data_filt, STATE_ABBR)   # varied top 10K - 15K
count(rad_data_filt, LOC_NUM)
count(rad_data_filt, ANA_UNIT)     # M3 followed by L
count(rad_data_filt, RESULT_UNIT)  # PCI/M3 followed by PCI/L
 
#MAT_ID, count of observations Fill by MAT_ID
ggplot(rad_data_filt, aes(RESULT_DATE, fill = MAT_ID)) + 
  geom_histogram(bins = 50)
ggplot(rad_data_filt, aes(MAT_ID)) + 
  geom_bar()
ggplot(rad_data_filt, aes(CITY_NAME)) + 
  geom_bar()

#Analytes, count of analytes
analytes_rad <- rad_data_filt %>% group_by(ANALYTE_ID) %>% 
  summarise(number = n())
summary(analytes_rad)
# test <- analytes_rad[order(analytes_rad$number),]

ggplot(HI_data_filt, aes(ANALYTE_ID)) + geom_bar()

#Locations: Cities and States
head(filter(rad_data_filt, STATE_ABBR == "ON"), 3)


#Result Data
ggplot(HI_data_filt, aes(x=RESULT_DATE, y= RESULT_AMOUNT, colour = ANALYTE_ID))+ geom_point()
ggplot(subset(HI_data_filt, RESULT_UNIT == "PCI/L"),aes(x=RESULT_DATE, y= RESULT_AMOUNT, color = MAT_ID)) +
  geom_point()
ggplot(subset(HI_data_filt, RESULT_UNIT == "PCI/M3" & ANALYTE_ID =="BETA"),
       aes(x=RESULT_DATE, y= RESULT_AMOUNT, color = ANALYTE_ID)) + 
       geom_point()


###Test plots
ggplot(HI_data, aes(RESULT_DATE, fill = ANALYTE_ID))+ geom_histogram() +
  facet_wrap(~HI_data$CITY_NAME)
ggplot(HI_data, aes(RESULT_DATE, fill = ANALYTE_ID))+ geom_histogram() +
  facet_wrap(~HI_data$MAT_ID)
ggplot(subset(HI_data, HI_data$ANALYTE_ID =="BETA"), 
       aes(RESULT_AMOUNT, fill = RESULT_UNIT)) +   
       geom_histogram(bins = 15)
ggplot(subset(HI_data, HI_data$RESULT_UNIT =="PCI/L"), 
       aes(RESULT_AMOUNT, fill = MAT_ID)) +   
       geom_histogram(bins = 15)
summary(HI_data$RESULT_AMOUNT)

ggplot(HI_data, aes(table, price, color = cut)) +
    geom_point()+
    scale_color_brewer(type='qual')+
  scale_x_continuous(name ="Table", breaks=seq(50,80,2), limits = c(50,80))
```
```{r}

#
```




```{r}

##CSU Combined Standard Uncertainty (95% = 2xCSU)
summary(rad_data_filt$CSU)
ggplot(subset(rad_data_filt, RESULT_UNIT == "PCI/L"), aes(x=RESULT_DATE, y= CSU, color = ANALYTE_ID)) +
  geom_point() +
  labs(title = "CSU by year",
       y="pCi/L", x =  "YEAR")

##MDC Minimum Detectable Concentration
summary(rad_data_filt$MDC)
ggplot(rad_data_filt, aes(x=RESULT_DATE, y= MDC, color = ANALYTE_ID)) +
  geom_point() +
  labs(title = "MDC by year",
       y="all units", x =  "YEAR")

ggplot(subset(rad_data_filt, !is.na(RESULT_UNIT) & RESULT_UNIT == "PCI/L"), aes(x=RESULT_DATE, y= MDC, color = ANALYTE_ID)) +
  geom_point() +
  labs(title = "MDC (pCi/L) by year",
       y="pCi/L", x =  "YEAR")


## Half-Life
summary(rad_data_filt$HALF_LIFE)
count(rad_data_filt, HALF_LIFE_TIME_UNIT)

ggplot(rad_data_filt, aes(x=RESULT_DATE, y= HALF_LIFE, color = ANALYTE_ID)) +
  geom_point() +
  labs(title = "Half-Lives of Analytes",
       y="all units", x =  "YEAR")
ggplot(subset(rad_data_filt, HALF_LIFE_TIME_UNIT =="Y"), aes(x=RESULT_DATE, y= HALF_LIFE, color = ANALYTE_ID)) +
  geom_point() +
  labs(title = "Half-Lives of Analytes",
       y="Half-life Years", x =  "YEAR")

##Duration (count time in minutes)
summary(rad_data_filt$DURATION)
count(rad_data_filt, DURATION)

rad_data_filt$LOC_NUM <- as.numeric(rad_data_filt$LOC_NUM)
hist(rad_data_filt$LOC_NUM)

#  geom_histogram() +
#  labs(title = "Testing Duration",
#       y="?", x =  "Duration")
```
```{r}
ggplot(rad_data_filt, aes(LOC_NUM)) +
  geom_histogram()
```

# Univariate Analysis

> **Tip**: Now that you've completed your univariate explorations, it's time to
reflect on and summarize what you've found. Use the questions below to help you
gather your observations and add your own if you have other thoughts!

### What is the structure of your dataset?

### What is/are the main feature(s) of interest in your dataset?

### What other features in the dataset do you think will help support your \
investigation into your feature(s) of interest?

### Did you create any new variables from existing variables in the dataset?

### Of the features you investigated, were there any unusual distributions? \
Did you perform any operations on the data to tidy, adjust, or change the form \
of the data? If so, why did you do this?


# Bivariate Plots Section

> **Tip**: Based on what you saw in the univariate plots, what relationships
between variables might be interesting to look at in this section? Don't limit
yourself to relationships between a main output feature and one of the
supporting variables. Try to look at relationships between supporting variables
as well.

```{r echo=FALSE, Bivariate_Plots}

```

# Bivariate Analysis

> **Tip**: As before, summarize what you found in your bivariate explorations
here. Use the questions below to guide your discussion.

### Talk about some of the relationships you observed in this part of the \
investigation. How did the feature(s) of interest vary with other features in \
the dataset?

### Did you observe any interesting relationships between the other features \
(not the main feature(s) of interest)?

### What was the strongest relationship you found?


# Multivariate Plots Section

> **Tip**: Now it's time to put everything together. Based on what you found in
the bivariate plots section, create a few multivariate plots to investigate
more complex interactions between variables. Make sure that the plots that you
create here are justified by the plots you explored in the previous section. If
you plan on creating any mathematical models, this is the section where you
will do that.

```{r echo=FALSE, Multivariate_Plots}

```

# Multivariate Analysis

### Talk about some of the relationships you observed in this part of the \
investigation. Were there features that strengthened each other in terms of \
looking at your feature(s) of interest?

### Were there any interesting or surprising interactions between features?

### OPTIONAL: Did you create any models with your dataset? Discuss the \
strengths and limitations of your model.

------

# Final Plots and Summary

> **Tip**: You've done a lot of exploration and have built up an understanding
of the structure of and relationships between the variables in your dataset.
Here, you will select three plots from all of your previous exploration to
present here as a summary of some of your most interesting findings. Make sure
that you have refined your selected plots for good titling, axis labels (with
units), and good aesthetic choices (e.g. color, transparency). After each plot,
make sure you justify why you chose each plot by describing what it shows.

### Plot One
```{r echo=FALSE, Plot_One}

```

### Description One


### Plot Two
```{r echo=FALSE, Plot_Two}

```

### Description Two


### Plot Three
```{r echo=FALSE, Plot_Three}

```

### Description Three

------

# Reflection

> **Tip**: Here's the final step! Reflect on the exploration you performed and
the insights you found. What were some of the struggles that you went through?
What went well? What was surprising? Make sure you include an insight into
future work that could be done with the dataset.

> **Tip**: Don't forget to remove this, and the other **Tip** sections before
saving your final work and knitting the final report!
